## Домашнее задание к уроку 1: Основы PyTorch

## Задание 1: Создание и манипуляции с тензорами
### 1.1 Создание тензоров

Условие задания:

```
# Создайте следующие тензоры:
# - Тензор размером 3x4, заполненный случайными числами от 0 до 1
# - Тензор размером 2x3x4, заполненный нулями
# - Тензор размером 5x5, заполненный единицами
# - Тензор размером 4x4 с числами от 0 до 15 (используйте reshape)
```

Мой код: 

```python
t1 = torch.rand(3, 4)
t2 = torch.zeros(2, 3, 4)
t3 = torch.ones(5, 5)
t4 = torch.arange(16).reshape(4, 4)
```

Выход функции:

```
tensor([[0.5326, 0.5707, 0.6388, 0.5265],
        [0.4969, 0.2292, 0.7357, 0.7930],
        [0.8619, 0.3834, 0.9340, 0.0445]])
tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]])
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15]])
```

## 1.2 Операции с тензорами

Условие задачи:

```
# Дано: тензор A размером 3x4 и тензор B размером 4x3
# Выполните:
# - Транспонирование тензора A
# - Матричное умножение A и B
# - Поэлементное умножение A и транспонированного B
# - Вычислите сумму всех элементов тензора A
```

Мой код:

```python
A = torch.randn(3, 4, device=device)
B = torch.randn(4, 3, device=device)
A_T = A.T
AB = torch.matmul(A, B)
A_BT = A * B.T
sum_A = A.sum()
```

Выход функции для входных тензоров:

```
tensor([[-1.1296, -0.9199, -1.4742, -0.9639],
        [-0.8116, -1.0187, -0.6496,  1.0513],
        [-1.2724,  1.0353, -0.8196,  0.9782]])
tensor([[ 0.1690, -0.1036,  0.4900],
        [-1.1874,  2.3180,  0.1757],
        [ 0.2826, -1.4431, -1.1483],
        [-0.3924, -0.0712, -0.7005]])
tensor([[-1.1296, -0.8116, -1.2724],
        [-0.9199, -1.0187,  1.0353],
        [-1.4742, -0.6496, -0.8196],
        [-0.9639,  1.0513,  0.9782]])
tensor([[ 0.8630,  0.1808,  1.6530],
        [ 0.4763, -1.4146, -0.5672],
        [-2.0597,  3.6448, -0.1856]])
tensor([[-0.1909,  1.0922, -0.4166,  0.3782],
        [ 0.0841, -2.3613,  0.9374, -0.0748],
        [-0.6234,  0.1819,  0.9412, -0.6852]])
tensor(-5.9946)
```

## 1.3 Индексация и срезы

Условие задачи:

```
# Создайте тензор размером 5x5x5
# Извлеките:
# - Первую строку
# - Последний столбец
# - Подматрицу размером 2x2 из центра тензора
# - Все элементы с четными индексами
```

Мой код:

```python
tensor_5x5x5 = torch.arange(125, device=device).reshape(5, 5, 5)
first_row = tensor_5x5x5[0, :, :]
last_col = tensor_5x5x5[:, :, -1]
center_sub = tensor_5x5x5[2:4, 2:4, 2:4]
even_indices = tensor_5x5x5[::2, ::2, ::2]
```

Выход функции:

```
tensor([[[ 0,  1,  2,  3,  4],
         [ 5,  6,  7,  8,  9],
         [10, 11, 12, 13, 14],
         [15, 16, 17, 18, 19],
         [20, 21, 22, 23, 24]],

        [[25, 26, 27, 28, 29],
         [30, 31, 32, 33, 34],
         [35, 36, 37, 38, 39],
         [40, 41, 42, 43, 44],
         [45, 46, 47, 48, 49]]])
tensor([[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19],
        [20, 21, 22, 23, 24]])
tensor([[  4,   9,  14,  19,  24],
        [ 29,  34,  39,  44,  49],
        [ 54,  59,  64,  69,  74],
        [ 79,  84,  89,  94,  99],
        [104, 109, 114, 119, 124]])
tensor([[[62, 63],
         [67, 68]],

        [[87, 88],
         [92, 93]]])
tensor([[[  0,   2,   4],
         [ 10,  12,  14],
         [ 20,  22,  24]],

        [[ 50,  52,  54],
         [ 60,  62,  64],
         [ 70,  72,  74]],

        [[100, 102, 104],
         [110, 112, 114],
         [120, 122, 124]]])
```

## 1.4 Работа с формами

Условия задачи:

```
# Создайте тензор размером 24 элемента
# Преобразуйте его в формы:
# - 2x12
# - 3x8
# - 4x6
# - 2x3x4
# - 2x2x2x3
```

Мой код:

```python
tensor_24 = torch.arange(24, device=device)
t_2x12 = tensor_24.reshape(2, 12)
t_3x8 = tensor_24.reshape(3, 8)
t_4x6 = tensor_24.reshape(4, 6)
t_2x3x4 = tensor_24.reshape(2, 3, 4)
t_2x2x2x3 = tensor_24.reshape(2, 2, 2, 3)
```

Выход функции:

```
tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],
        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])
tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],
        [ 8,  9, 10, 11, 12, 13, 14, 15],
        [16, 17, 18, 19, 20, 21, 22, 23]])
tensor([[ 0,  1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10, 11],
        [12, 13, 14, 15, 16, 17],
        [18, 19, 20, 21, 22, 23]])
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
tensor([[[[ 0,  1,  2],
          [ 3,  4,  5]],

         [[ 6,  7,  8],
          [ 9, 10, 11]]],


        [[[12, 13, 14],
          [15, 16, 17]],

         [[18, 19, 20],
          [21, 22, 23]]]])
```

## 2.1 Простые вычисления с градиентами

Условие задачи:

```
# Создайте тензоры x, y, z с requires_grad=True
# Вычислите функцию: f(x,y,z) = x^2 + y^2 + z^2 + 2*x*y*z
# Найдите градиенты по всем переменным
# Проверьте результат аналитически
```

Ход решения: Создала три тензора размером 2×2 с включённым отслеживанием градиентов. Вычислила функцию по формуле. Использовала метод `.backward()` для автоматического вычисления градиентов. Для проверки вычислила аналитические формулы частных производных и сравнила с результатами PyTorch.

Мой код:

```python
x = torch.tensor([[4.0, 6.0], [8.0, 10.0]], requires_grad=True)
y = torch.tensor([[2.0, 4.0], [6.0, 8.0]], requires_grad=True)
z = torch.tensor([[3.0, 4.0], [5.0, 6.0]], requires_grad=True)

f = x**2 + y**2 + z**2 + 2 * x * y * z
f.sum().backward()
```

Выход функции:

```
x.grad = tensor([[ 20.,  44.],
        [ 76., 116.]])
y.grad = tensor([[ 28.,  56.],
        [ 92., 136.]])
z.grad = tensor([[ 22.,  56.],
        [106., 172.]])
2*x + 2*y*z = tensor([[ 20.,  44.],
        [ 76., 116.]], grad_fn=<AddBackward0>)
2*y + 2*x*z = tensor([[ 28.,  56.],
        [ 92., 136.]], grad_fn=<AddBackward0>)
2*z + 2*x*y = tensor([[ 22.,  56.],
        [106., 172.]], grad_fn=<AddBackward0>)
```

## 2.2 Градиент функции потерь

Условие задачи:

```
# Реализуйте функцию MSE (Mean Squared Error):
# MSE = (1/n) * Σ(y_pred - y_true)^2
# где y_pred = w * x + b (линейная функция)
# Найдите градиенты по w и b
```

Ход решения: Создала синтетические данные: матрицу признаков X размером 5×3 и вектор целевых значений y_true. Инициализировала веса w и смещение b со случайными значениями. Реализовала линейную модель и вычислила MSE-функцию потерь. Использовала .backward() для получения градиентов. Для проверки вычислила градиенты по аналитическим формулам.

Мой код: 

```python
n, m = 5, 3
X = torch.tensor([[1.0, 2.0, 3.0],
                  [4.0, 5.0, 6.0],
                  [7.0, 8.0, 9.0],
                  [10.0, 11.0, 12.0],
                  [13.0, 14.0, 15.0]])
y_true = torch.tensor([[2.0], [5.0], [8.0], [11.0], [14.0]])

w = torch.tensor([[0.5], [1.0], [1.5]], requires_grad=True)
b = torch.tensor([0.1], requires_grad=True)

y_pred = X @ w + b
mse_loss = torch.mean((y_pred - y_true) ** 2)
mse_loss.backward()
```

Выход функции:

```
w.grad = tensor([[311.4000],
        [345.6000],
        [379.8000]])
b.grad = tensor([34.2000])
(2/n) * X.T @ (X @ w + b - y_true) = tensor([[311.4000],
        [345.6000],
        [379.8000]], grad_fn=<MmBackward0>)
(2/n) * torch.sum(X @ w + b - y_true) = tensor(34.2000, grad_fn=<MulBackward0>)
```

## 2.3 Цепное правило

Условия задачи:

```
# Реализуйте составную функцию: f(x) = sin(x^2 + 1)
# Найдите градиент df/dx
# Проверьте результат с помощью torch.autograd.grad
```

Ход решения:  Создала тензор x с включённым отслеживанием градиентов. Реализовала составную функцию sin(x²+1). Вычислила градиент с помощью .backward(). Для проверки использовала функцию torch.autograd.grad().

Мой код: 

```python
x_chain = torch.tensor([[1.0, 2.0, 3.0],
                        [4.0, 5.0, 6.0],
                        [7.0, 8.0, 9.0]], requires_grad=True)

f_chain = torch.sin(x_chain**2 + 1)
f_chain.sum().backward(retain_graph=True)
grad_check = torch.autograd.grad(outputs=f_chain.sum(), inputs=x_chain)[0]
```

Выходы функции:

```
x.grad = tensor([[-0.8323,  1.1346, -5.0344],
        [-2.2013,  6.4692,  9.1850],
        [13.5095, -8.9993, 17.0942]])
torch.autograd.grad = tensor([[-0.8323,  1.1346, -5.0344],
        [-2.2013,  6.4692,  9.1850],
        [13.5095, -8.9993, 17.0942]])

```

## 3.1 Подготовка данных

Условие задачи:

```
# Создайте большие матрицы размеров:
# - 64 x 1024 x 1024
# - 128 x 512 x 512
# - 256 x 256 x 256
# Заполните их случайными числами
```

Мой код:

```python
matrix_64 = torch.randn(64, 1024, 1024)
matrix_128 = torch.randn(128, 512, 512)
matrix_256 = torch.randn(256, 256, 256)
```

## 3.2 Функция измерения времени

Условие задачи:
```
# Создайте функцию для измерения времени выполнения операций
# Используйте torch.cuda.Event() для точного измерения на GPU
# Используйте time.time() для измерения на CPU
```

Функция для измерения времени:
```python
def measure_time(operation, matrix, use_gpu=False):
    if use_gpu and has_cuda:
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)
        start_event.record()
        result = operation(matrix)
        end_event.record()
        torch.cuda.synchronize()
        elapsed_time = start_event.elapsed_time(end_event)
    else:
        start_time = time.time()
        result = operation(matrix)
        elapsed_time = (time.time() - start_time) * 1000

    return result, elapsed_time
```

## 3.3 Сравнение операций

Условие задачи:

```
# Сравните время выполнения следующих операций на CPU и CUDA:
# - Матричное умножение (torch.matmul)
# - Поэлементное сложение
# - Поэлементное умножение
# - Транспонирование
# - Вычисление суммы всех элементов

# Для каждой операции:
# 1. Измерьте время на CPU
# 2. Измерьте время на GPU (если доступен)
# 3. Вычислите ускорение (speedup)
# 4. Выведите результаты в табличном виде
```

Таблица результатов:
```
Операция                  | CPU (мс)   | GPU (мс)   | Ускорение 
Матричное умножение       | 432.25     | N/A        | N/A       
Сложение                  | 70.35      | N/A        | N/A       
Умножение                 | 33.22      | N/A        | N/A       
Транспонирование          | 0.00       | N/A        | N/A       
Сумма элементов           | 6.60       | N/A        | N/A   
```


## 3.4 Анализ результатов:

 1) Наибольшее ускорение на GPU получает матричное умножение, так как эта операция идеально параллелизуется и эффективно использует тысячи потоковых процессоров GPU. Поэлементные операции тоже ускоряются значительно, но меньше.
 2) Операции могут быть медленнее на GPU из-за накладных расходов на передачу данных между CPU и GPU, задержек при запуске вычислений на GPU, а также из-за того, что для маленьких матриц эти накладные расходы перевешивают выигрыш от параллельных вычислений.
 3) Чем больше размер матрицы, тем больше ускорение на GPU. Большие матрицы (от 1024×1024) дают максимальное ускорение, потому что GPU может полностью загрузиться параллельными вычислениями. Маленькие матрицы (меньше 128×128) могут даже работать медленнее на GPU из-за накладных расходов.
 4) Данные копируются через шину PCI Express, что создаёт задержку и ограничивает скорость. Если передавать данные туда-обратно для каждой операции, это может съесть весь выигрыш от GPU. Поэтому лучше выполнять несколько операций подряд на GPU, а потом копировать результаты обратно.